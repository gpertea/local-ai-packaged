volumes:
  n8n_storage:
  # ollama_storage: # Removed
  qdrant_storage:
  open-webui:
  flowise:
  # caddy-data: # Removed
  # caddy-config: # Removed
  valkey-data:

x-n8n: &service-n8n
  image: n8nio/n8n:latest
  environment:
    - DB_TYPE=postgresdb
    - DB_POSTGRESDB_HOST=db # Assumes 'db' is the service name for Postgres within the Supabase stack
    - DB_POSTGRESDB_USER=postgres
    - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD} # From .env
    - DB_POSTGRESDB_DATABASE=postgres
    - N8N_DIAGNOSTICS_ENABLED=false
    - N8N_PERSONALIZATION_ENABLED=false
    - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY} # From .env
    - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET} # From .env
    # - EXTERNAL_LLM_URL=${EXTERNAL_LLM_URL} # Example: Add external URL from .env if needed globally
    # Note: n8n LLM connections are usually configured via Credentials in the UI

services:
  flowise:
    image: flowiseai/flowise
    restart: unless-stopped
    container_name: flowise
    environment:
        - PORT=3001
        # Configure LLM nodes within Flowise UI to point to your external LLM URL (e.g., http://<LAN_IP_of_LLM>:11434)
    ports:
        - 3001:3001 # Mapped for NPM access
    extra_hosts:
      # Optional: Add mapping if your LLM host needs a specific hostname resolution from container
      # - "llm.host.lan:<LAN_IP_of_LLM>" 
      - "host.docker.internal:host-gateway" 
    volumes:
        - flowise:/root/.flowise # Changed from ~/.flowise to use named volume
    entrypoint: /bin/sh -c "sleep 3; flowise start"

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    container_name: open-webui
    environment:
      # Configure OpenWebUI to point to your external LLM URL via its specific env vars
      # Example (Check OpenWebUI docs for exact variable names):
      - OLLAMA_BASE_URL=${EXTERNAL_LLM_URL} # Use external URL from .env
      - ENABLE_SIGNUP=true 
    ports:
      - "3000:8080" # Mapped for NPM access
    extra_hosts:
      # Optional: Add mapping if your LLM host needs a specific hostname resolution from container
      # - "llm.host.lan:<LAN_IP_of_LLM>" 
      - "host.docker.internal:host-gateway"
    volumes:
      - open-webui:/app/backend/data

  n8n-import:
    <<: *service-n8n
    container_name: n8n-import
    entrypoint: /bin/sh
    command:
      - "-c"
      - "n8n import:credentials --separate --input=/backup/credentials && n8n import:workflow --separate --input=/backup/workflows"
    volumes:
      - ./n8n/backup:/backup

  n8n:
    <<: *service-n8n
    container_name: n8n
    restart: unless-stopped
    ports:
      - 5678:5678 # Mapped for NPM access
    volumes:
      - n8n_storage:/home/node/.n8n
      - ./n8n/backup:/backup
      - ./shared:/data/shared
    depends_on:
      n8n-import:
        condition: service_completed_successfully
      # db: # Ensure n8n waits for the Supabase DB service ('db')
      #  condition: service_healthy 

  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    restart: unless-stopped
    ports:
      - 6333:6333 # Keep if accessed directly from host, otherwise remove if only internal
    volumes:
      - qdrant_storage:/qdrant/storage

  # caddy: Removed

  redis: # Used by SearXNG
    container_name: redis
    image: docker.io/valkey/valkey:8-alpine
    command: valkey-server --save 30 1 --loglevel warning
    restart: unless-stopped
    volumes:
      - valkey-data:/data
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    # ports: # Removed - Internal access only
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      # - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/ # Removed Caddy dependency
      - SEARXNG_BASE_URL=http://searxng:8080/ # Optional: Set fixed internal URL if needed by other containers
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
    cap_drop:
      - ALL 
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    depends_on:
      - redis

# ollama-* services: Removed
# ollama-pull-llama-* services: Removed
